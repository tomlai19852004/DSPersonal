https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4?source=---
Beside’s Kai’s, I’ve seen one more implementation from a PhD student named Satoshi: a brief example in Python with Chainer. I haven’t spent as much time with it, as I had to adapt it to run on my CPU due to lack of memory. But I did notice:It uses content to style ratios in a more similar range to the original paper. Changing this by an order of magnitude doesn’t seem to have as big an effect.It tries to initialize with noise.It uses VGG-19.After running Tübingen in the style of The Starry Night with a 1:10e3 ratio and 100 iterations, it seems to converge on something matching the general structure but lacking the overall palette:Generated using Satoshi’s example.I’d like to understand this algorithm well enough to generalize it to other media (mainly thinking about sound right now), so if you have an insights or other implementations please share them in the comments!UpdateI’ve started testing another implementation that popped up this morning from Justin Johnson. His follow the original paper very closely, except for using unequal weights when balancing different layers used for style reconstruction. All the following examples were run for 100 iterations with the default ratio of 1:10e0.In the style of “Composition VII” by Kandinsky.In the style of “The Scream” by Munch.In the style of “Seated Nude” by PicassoIn the style of “The Shipwreck of the Minotaur” by TurnerIn the style of “The Starry Night” by GoghGandalf in the style of “A Muse” by PicassoFinal UpdateJustin switched his implementation to use L-BFGS and equally weighted layers, and to my eyes this matches the results in the original paper. Here are his results for one of the harder content/style pairs:In the style of “Seated Nude” by PicassoOther implementations that look great, but I haven’t tested enough:neural_artistic_style by Anders Boesen Lindbo Larsen, in Python using DeepPy and cuDNN. Example images look great, and are very high resolution.styletransfer by Eben Olson, in Python using Lasagne. Replicates both style transfer and style reconstruction from noise.FootnotesThe definition of the Gram matrix confused me at first, so I wrote it out as code. Using a literal translation of equation 3 in the paper, you would write in Python, with numpy:def gram(layer):  N = layer.shape[1]  F = layer.reshape(N, -1)  M = F.shape[1]  G = np.zeros((N, N))  for i in range(N):    for j in range(N):      for k in range(M):        G[i,j] += F[i,k] * F[j,k]  return GIt turns out that the original description is computed more efficiently than this literal translation. For example, Kai writes in Lua, with Torch:function gram(input)  local k = input:size(2)  local flat = input:view(k, -1)  local gram = torch.mm(flat, flat:t())  return gramendSatoshi computes it for all the layers simultaneously in Python with Chainer:conv1_1F,conv2_1F,conv3_1F,conv4_1F,conv5_1F, = [ reshape2(x) for x in [conv1_1,conv2_1, conv3_1, conv4_1,conv5_1]]conv1_1G,conv2_1G,conv3_1G,conv4_1G,conv5_1G, = [ Fu.matmul(x, x, transa=False, transb=True) for x in [conv1_1F,conv2_1F, conv3_1F, conv4_1F,conv5_1F]]Or again in Python, with numpy and Caffe layers:def gram(x):  F = layer.reshape(layer.shape[1], -1)  return np.dot(F, F.T)